from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession, Row
import requests
from pprint import pprint
from pymongo import MongoClient
import json


sc = SparkContext("local[2]", "NEW_Y_TIMES_PROJET")
sc.setLogLevel("ERROR")
ssc = StreamingContext(sc, 10)

spark = SparkSession \
    .builder \
    .master("local") \
    .appName("NEW_Y_TIMES_PROJET") \
    .getOrCreate()



cols = ['uri','multimedia','thumbnail_standard','geo_facet','per_facet','org_facet','des_facet','related_urls','short_url','subheadline']
section = "all"
key = "nh8jOjOc8udr9E04wrwpJA6s3VBECjh4"


def extract(key, section):

  """
  Cette fonction prend la clé de l'API et une section et donne en sortie une liste d'article correspondant à la section

  EXEMPLE D'APPEL : articles = getArticles(APIKEY, sections[1])
  """  

  #Nous recupérons nos articles directement du new york times
  nyt = "nyt"

  #Utilisation de la library request pour récupérer les éléments de l'URL (code récupérer sur NY TIMES DEVELOPER)

  #Instanciation des variables pour récupérer les données avec l'API
  requestUrl = f"https://api.nytimes.com/svc/news/v3/content/{nyt}/{section}.json?api-key={key}&limit=500"
  requestHeaders = {
    "Accept": "application/json"
  }
  response = requests.get(requestUrl, headers=requestHeaders)

  #transformation des données json en JS qui est un fichier manipulable
  #data = response.json()
  data = json.loads(response.text)

  articles = data.get("results")
  #on passe la sous partie results de la réponse obtenue car c'est ici que se trouve les articles
  #results = data["results"]
  
  for article in articles:
    selected = {k: article.get(k,"") for k in cols}
    yield selected

  #creation d'une liste avec les colonnes qui seront supprimés pour chaque article
  #cols = ['uri','multimedia','thumbnail_standard','geo_facet','per_facet','org_facet','des_facet','related_urls','short_url','subheadline']

  #Boucle pour supprimer les colonnes 
  #for article in results:
        #article = [article.pop(c, None) for c in cols]
  
  #print(type(results[0]))
  #return articles

dstream = ssc.queueStream([extract(key, section)])


def processData(rdd):

    if not rdd.isEmpty():
        rows = [Row(**k) for k in rdd.collect()]
        df = spark.createDataFrame(rows)
        #on call la transformation
        df.show()
        #mafonction load
    else: 
        print('le RDD est vide voir la partie extraction !')


dstream.foreachRDD(processData)


ssc.start()
ssc.awaitTermination()


#pprint(extract(key, section))